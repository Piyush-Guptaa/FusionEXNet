{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqc4Dges5loG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oo6rxdgJ1hnk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import sklearn\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.applications import *\n",
        "from keras.layers import *\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.utils import class_weight\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.xception import preprocess_input as base_preprocess\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, RocCurveDisplay, auc\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjcNg8hCogk0"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UILhBw1hIEsB"
      },
      "outputs": [],
      "source": [
        "# Load your data\n",
        "data = np.load('/content/drive/MyDrive/XAIIIII/PreprocessedData-Final.npz')\n",
        "images_preprocessed = data['images']\n",
        "labels_onehot = data['labels']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "MqT7cdpSQ1Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ORtFoSoD8er"
      },
      "outputs": [],
      "source": [
        "print(\"Data shape:\", data.shape)\n",
        "print(\"Labels shape:\", labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bn-VaEQD5Y1"
      },
      "source": [
        "Loading Images from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2JxI7wK7mwC"
      },
      "outputs": [],
      "source": [
        "unique_classes = np.unique(np.argmax(labels_onehot, axis=1))\n",
        "selected_images = []\n",
        "selected_labels = []\n",
        "\n",
        "for cls in unique_classes:\n",
        "    # Find the first occurrence of each class in the test dataset\n",
        "    idx = np.where(np.argmax(test_labels, axis=1) == cls)[0][0]\n",
        "    selected_images.append(test_data[idx])\n",
        "    selected_labels.append(test_labels[idx])\n",
        "\n",
        "selected_images = np.array(selected_images)\n",
        "# Class names mapping\n",
        "class_names = {0: \"AKIEC\", 1: \"BCC\", 2: \"BKL\", 3: \"DF\", 4: \"MEL\", 5: \"NV\", 6: \"VASC\"}\n",
        "\n",
        "# Adjusting plot sizes and layout for a single row\n",
        "num_images = len(selected_images)\n",
        "num_cols = num_images  # Number of columns is equal to the number of images\n",
        "num_rows = 1  # All images in one row\n",
        "\n",
        "plt.figure(figsize=(2 * num_cols, 2.5 * num_rows))  # Adjust the figure size so each image has enough space\n",
        "\n",
        "for i, image in enumerate(selected_images):\n",
        "    plt.subplot(num_rows, num_cols, i + 1)\n",
        "    plt.imshow(image)\n",
        "    class_idx = np.argmax(selected_labels[i])\n",
        "    plt.title(f'{class_names[class_idx]}')  # Use class names mapping here\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzx3OjvsD_Vm"
      },
      "source": [
        "Frequency of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdjVVOV89tMv"
      },
      "outputs": [],
      "source": [
        "# Sum the one-hot encoded labels along the rows to get the frequency of each class\n",
        "class_counts = np.sum(labels, axis=0)\n",
        "\n",
        "# Map class indices to their corresponding names\n",
        "class_names = {0: \"AKIEC\", 1: \"BCC\", 2: \"BKL\", 3: \"DF\", 4: \"MEL\", 5: \"NV\", 6: \"VASC\"}\n",
        "\n",
        "# Plot the class frequencies\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar([class_names[class_idx] for class_idx in range(len(class_names))], class_counts)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Images in each class')\n",
        "\n",
        "# Annotate the bars with the class frequencies (integer format)\n",
        "for i, count in enumerate(class_counts):\n",
        "    plt.text(i, count, str(int(count)), ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBqg0HtzolJf"
      },
      "source": [
        "Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJtRjtyn10ds"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    images_preprocessed, labels_onehot, test_size=0.1, stratify=labels_onehot, random_state=42)\n",
        "\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    train_data, train_labels, test_size=0.1, stratify=train_labels, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhgHZsD0orBE"
      },
      "source": [
        "Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqVPpQqi5oAE"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjFsGNxEpOJ4"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2ZWA_ZW2o0d"
      },
      "outputs": [],
      "source": [
        "lr_reduce = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.5, patience = 5,mode='max', min_lr = 1e-4,verbose = 1)\n",
        "checkpoint_filepath = '/content/drive/MyDrive/XAIIIII/Saved Model/Fusion/FusionEXNet.h5'\n",
        "model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,save_best_only = True, monitor = 'val_accuracy',verbose = 1)\n",
        "\n",
        "callback_list = [model_checkpoint_callback, lr_reduce]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlPk6ud13wRG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import EfficientNetV2S, Xception\n",
        "\n",
        "# Input layer\n",
        "input_tensor = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Base models\n",
        "base_model_1 = EfficientNetV2S(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "base_model_2 = Xception(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# Make sure base models are not trainable to freeze their weights\n",
        "base_model_1.trainable = False\n",
        "base_model_2.trainable = False\n",
        "\n",
        "# Features from both models\n",
        "features1 = base_model_1.output\n",
        "features2 = base_model_2.output\n",
        "\n",
        "# Global Average Pooling applied to both sets of features\n",
        "gap1 = GlobalAveragePooling2D()(features1)\n",
        "gap2 = GlobalAveragePooling2D()(features2)\n",
        "\n",
        "# Concatenate the features from both models\n",
        "concatenated_features = Concatenate()([gap1, gap2])\n",
        "\n",
        "# Classification layers\n",
        "x = Dense(128, activation='relu')(concatenated_features)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(7, activation='softmax')(x)\n",
        "\n",
        "# Combined model\n",
        "model = Model(inputs=input_tensor, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAHmXYMF4QYy"
      },
      "outputs": [],
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(7, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNBcG8NL4RoB"
      },
      "outputs": [],
      "source": [
        "epochs = 55\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=train_data, y=train_labels,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(val_data, val_labels),\n",
        "                    epochs=epochs,\n",
        "                    callbacks=callback_list)"
      ],
      "metadata": {
        "id": "a8O3wmrHRZzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the loss curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss, label='Training Loss')\n",
        "plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting the accuracy curves\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, accuracy, label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JY6yTdABTk4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIwFVnoNFLYY"
      },
      "outputs": [],
      "source": [
        "model= load_model('/content/drive/MyDrive/XAIIIII/Saved Model/Fusion/FusedXEfficient.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMDcbEoKpXXJ"
      },
      "source": [
        "Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "AHl8ooLAV4sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG6J6bKppaGt"
      },
      "source": [
        "Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziX3qIzHXLIQ"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Convert predictions and true labels to integer format\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Calculate the classification report\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-ZhmrFJpfq-"
      },
      "source": [
        "Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp7qRDV5n5-8"
      },
      "outputs": [],
      "source": [
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, np.round(predicted_labels))\n",
        "\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uPgVbNPn8U2"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    # print(cm)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "cm_plot_labels = [\"MEL\", \"NV\", \"BCC\", \"AKIEC\", \"BKL\", \"DF\", \"VASC\"]\n",
        "\n",
        "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix', normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ptGOPsrOl3"
      },
      "source": [
        "ROC-AUC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acduvDInq8Mw"
      },
      "outputs": [],
      "source": [
        "# Define class names\n",
        "class_names = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC']\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Get the number of classes\n",
        "num_classes = test_labels.shape[1]\n",
        "\n",
        "# Initialize a figure to plot ROC curves\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "# Loop through each class\n",
        "for class_index in range(num_classes):\n",
        "    # Compute ROC curve and ROC AUC for the current class\n",
        "    fpr, tpr, thresholds = roc_curve(test_labels[:, class_index], predictions[:, class_index])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve for the current class\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{class_names[class_index]} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Plot the diagonal line (random chance)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
        "\n",
        "# Set plot properties\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) - Multiclass')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiXc4q6WppTy"
      },
      "source": [
        "Misclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU5wn6OTkc11"
      },
      "outputs": [],
      "source": [
        "!pip install tf-keras-vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcwwy6HKk4qf"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from tensorflow.python.client import device_lib\n",
        "device_list = device_lib.list_local_devices()\n",
        "gpus = [device.name for device in device_list if device.device_type == 'GPU']\n",
        "print('TensorFlow recognized {} GPUs'.format(len(gpus)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYzUEbUwk_K9"
      },
      "outputs": [],
      "source": [
        "image_titles = ['AKIEC', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'VASC']\n",
        "num_images = len(image_titles)\n",
        "\n",
        "test_labels_int = np.argmax(test_labels, axis=1)\n",
        "# Find the indices of the first image from each class\n",
        "class_indices = [np.where(test_labels_int == i)[0][0] for i in range(len(image_titles))]\n",
        "\n",
        "# Create an array to store the images\n",
        "image_array = []\n",
        "\n",
        "# Create a single row of plots\n",
        "num_images = len(image_titles)\n",
        "fig, ax = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "\n",
        "for i, title in enumerate(image_titles):\n",
        "    ax[i].set_title(title, fontsize=12)\n",
        "    ax[i].axis('off')\n",
        "\n",
        "    # Display the image from test data\n",
        "    img = test_data[class_indices[i]]\n",
        "    image_array.append(img)  # Store the image in the array\n",
        "\n",
        "    ax[i].imshow(img)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "X = base_preprocess(np.array(image_array))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
        "\n",
        "replace2linear = ReplaceToLinear()\n",
        "\n",
        "# Instead of using the ReplaceToLinear instance above,\n",
        "# you can also define the function from scratch as follows:\n",
        "def model_modifier_function(cloned_model):\n",
        "    cloned_model.layers[-1].activation = tf.keras.activations.linear"
      ],
      "metadata": {
        "id": "5m0FQi0bUf8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tf_keras_vis.utils.scores import CategoricalScore\n",
        "\n",
        "score = CategoricalScore([0, 1, 2, 3, 4, 5, 6])\n",
        "\n",
        "def score_function(output):\n",
        "    return (output[0][1][2][3][4][5][6])"
      ],
      "metadata": {
        "id": "BasZfO6XUf_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from matplotlib import cm\n",
        "from tf_keras_vis.scorecam import Scorecam\n",
        "\n",
        "# Create ScoreCAM object\n",
        "scorecam = Scorecam(model, model_modifier=replace2linear)\n",
        "\n",
        "# Generate heatmap with Faster-ScoreCAM\n",
        "cam = scorecam(score,\n",
        "               X,\n",
        "               penultimate_layer=-1,\n",
        "               max_N=10)\n",
        "\n",
        "## Since v0.6.0, calling `normalize()` is NOT necessary.\n",
        "# cam = normalize(cam)\n",
        "\n",
        "# Calculate the number of rows and columns for subplots\n",
        "num_rows = 2\n",
        "num_cols = (num_images + 1) // num_rows\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(5, 3))\n",
        "\n",
        "for i, title in enumerate(image_titles):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "\n",
        "    heatmap = np.uint8(cm.jet(cam[i])[..., :3] * 255)\n",
        "    ax[row, col].set_title(title, fontsize=8)\n",
        "    ax[row, col].imshow(image_array[i])\n",
        "    ax[row, col].imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "    ax[row, col].axis('off')\n",
        "\n",
        "# Remove any empty subplots\n",
        "for i in range(len(image_titles), num_rows * num_cols):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "    f.delaxes(ax[row, col])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "REMWaPQ8VTK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from keras import backend as K\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "\n",
        "# Create Saliency object.\n",
        "saliency = Saliency(model,\n",
        "                    model_modifier=replace2linear,\n",
        "                    clone=True)\n",
        "\n",
        "# Generate saliency map with smoothing that reduce noise by adding noise\n",
        "saliency_map = saliency(score,\n",
        "                        X,\n",
        "                        smooth_samples=20, # The number of calculating gradients iterations.\n",
        "                        smooth_noise=0.20) # noise spread level.\n",
        "\n",
        "## Since v0.6.0, calling `normalize()` is NOT necessary.\n",
        "# saliency_map = normalize(saliency_map)\n",
        "\n",
        "# Calculate the number of rows and columns for subplots\n",
        "num_rows = 2\n",
        "num_cols = (num_images + 1) // num_rows\n",
        "\n",
        "# Render\n",
        "f, ax = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(5, 3))\n",
        "\n",
        "for i, title in enumerate(image_titles):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "\n",
        "    ax[row, col].set_title(title, fontsize=8)\n",
        "    ax[row, col].imshow(saliency_map[i], cmap='jet')\n",
        "    ax[row, col].axis('off')\n",
        "\n",
        "# Remove any empty subplots\n",
        "for i in range(len(image_titles), num_rows * num_cols):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "    f.delaxes(ax[row, col])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I66QWCs3UgPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mxAZ26AUgSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7YnCk5EUgU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}